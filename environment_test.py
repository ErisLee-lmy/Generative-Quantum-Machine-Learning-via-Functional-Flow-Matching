#!/usr/bin/env python3
import os
import sys
import subprocess
import platform
import traceback
import shutil

print("="*60)
print("ğŸ” PyTorch WSL2 å¿«é€Ÿè¯Šæ–­å·¥å…·")
print("="*60)

# 1. æ£€æŸ¥ Python ç‰ˆæœ¬ä¸è·¯å¾„
print("\n[1] Python ç¯å¢ƒæ£€æŸ¥")
print("- Python ç‰ˆæœ¬:", sys.version)
print("- Python è·¯å¾„:", sys.executable)

# 2. æ£€æŸ¥æ˜¯å¦åœ¨ /mnt/c/ ç›®å½•ä¸‹è¿è¡Œ
cwd = os.getcwd()
if cwd.startswith("/mnt/"):
    print(f"- å½“å‰è·¯å¾„åœ¨ Windows æŒ‚è½½ç›®å½•: {cwd}")
    print("  âš  å»ºè®®å°†ä»£ç ç§»è‡³ WSL å†…éƒ¨ ext4 æ–‡ä»¶ç³»ç»Ÿ (å¦‚ ~/project) æå‡ç¨³å®šæ€§")
else:
    print(f"- å½“å‰è·¯å¾„åœ¨ WSL å†…éƒ¨ç›®å½•: {cwd}")

# 3. æ£€æŸ¥æ˜¯å¦å®‰è£… PyTorch
print("\n[2] PyTorch å®‰è£…æ£€æŸ¥")
try:
    import torch
    print("- PyTorch ç‰ˆæœ¬:", torch.__version__)
    print("- å®‰è£…è·¯å¾„:", torch.__file__)
except Exception as e:
    print("âŒ æ— æ³•å¯¼å…¥ torch:", e)
    sys.exit(1)

# 4. æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
print("\n[3] CUDA æ£€æŸ¥")
try:
    cuda_avail = torch.cuda.is_available()
    print("- CUDA å¯ç”¨æ€§:", cuda_avail)
    if cuda_avail:
        print("- GPU æ•°é‡:", torch.cuda.device_count())
        print("- å½“å‰ GPU åç§°:", torch.cuda.get_device_name(0))
    else:
        print("  âš  æœªæ£€æµ‹åˆ° GPUï¼Œè‹¥éœ€ GPU éœ€æ£€æŸ¥ WSL CUDA é©±åŠ¨ä¸ wsl --update")
except Exception:
    traceback.print_exc()

# 5. æ£€æŸ¥çº¿ç¨‹åº“å†²çª
print("\n[4] çº¿ç¨‹åº“æ£€æŸ¥ (MKL/OpenBLAS)")
env_mkl = os.environ.get("MKL_THREADING_LAYER")
print("- MKL_THREADING_LAYER =", env_mkl if env_mkl else "æœªè®¾ç½®")
try:
    test_code = """import torch, os; os.environ['OMP_NUM_THREADS']='1'; os.environ['MKL_THREADING_LAYER']='GNU'; import torch; print('OK')"""
    subprocess.run([sys.executable, "-c", test_code], timeout=10, check=True)
    print("  âœ… è®¾ç½® OMP_NUM_THREADS=1 + MKL_THREADING_LAYER=GNU å¯¼å…¥æ­£å¸¸")
except subprocess.TimeoutExpired:
    print("  âŒ å¯¼å…¥ torch è¶…æ—¶ï¼Œå¯èƒ½çº¿ç¨‹åº“å†²çªæˆ–æ–‡ä»¶ç³»ç»Ÿå¡æ­»")

# 6. æ£€æŸ¥æ˜¯å¦å¤šç‰ˆæœ¬å†²çª
print("\n[5] æ£€æŸ¥å¤šç‰ˆæœ¬ Torch å®‰è£…")
pip_show = shutil.which("pip3")
if pip_show:
    try:
        pip_list = subprocess.check_output([pip_show, "show", "torch"]).decode()
        print(pip_list)
    except subprocess.CalledProcessError:
        print("æœªé€šè¿‡ pip å®‰è£… torch")
else:
    print("pip3 æœªæ‰¾åˆ°")

# 7. æ£€æŸ¥å†…æ ¸ä¿¡æ¯
print("\n[6] WSL å†…æ ¸ä¸ç³»ç»Ÿä¿¡æ¯")
print("- å¹³å°:", platform.platform())
try:
    uname = subprocess.check_output(["uname", "-a"]).decode().strip()
    print("- uname:", uname)
except Exception:
    pass

print("\nè¯Šæ–­å®Œæˆã€‚")
print("="*60)
print("å»ºè®®ï¼š")
print("1. å¦‚æœ CUDA ä¸å¯ç”¨ä¸”ä½ å®‰è£…äº† GPU ç‰ˆ PyTorchï¼Œè¯·é‡è£… CPU-only ç‰ˆæˆ–é…ç½® WSL CUDA")
print("2. å¦‚æœçº¿ç¨‹åº“å†²çªï¼Œå°è¯•åœ¨ä»£ç ä¸­è®¾ç½®:")
print("   os.environ['OMP_NUM_THREADS']='1'; os.environ['MKL_THREADING_LAYER']='GNU'")
print("3. å¦‚æœåœ¨ /mnt/c/ ä¸‹è¿è¡Œï¼Œç§»åŠ¨ä»£ç è‡³ WSL å†…éƒ¨ç›®å½•")
print("4. ç¡®ä¿ WSL2 ä¸ Windows é©±åŠ¨æ›´æ–°åˆ°æœ€æ–° (wsl --update)")
print("="*60)
